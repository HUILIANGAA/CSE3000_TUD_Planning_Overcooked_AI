{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import time, gym\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import seaborn\n",
    "import numpy as np\n",
    "from stable_baselines import GAIL\n",
    "from hr_coordination.utils import save_pickle, load_pickle, reset_tf, load_dict_from_file, create_dir_if_not_exists, profile, cross_entropy, accuracy\n",
    "from hr_coordination.agents.agent import AgentPair, RandomAgent, GreedyHumanModel, AgentFromPolicy, StayAgent, EmbeddedPlanningAgent\n",
    "from hr_coordination.agents.benchmarking import AgentEvaluator\n",
    "from hr_coordination.pbt.pbt_utils import get_config_from_pbt_dir, load_pickle, setup_mdp_env, get_vectorized_gym_env, create_model, update_model\n",
    "from hr_coordination.imitation.behavioural_cloning import train_bc_agent, get_bc_agent_from_saved, DEFAULT_DATA_PARAMS, DEFAULT_ENV_PARAMS, eval_with_benchmarking_from_model, eval_with_benchmarking_from_saved, BC_SAVE_DIR, get_bc_agent_from_model, symmetric_bc\n",
    "from hr_coordination.mdp.overcooked_mdp import Action, NO_REW_SHAPING_PARAMS, Direction, OvercookedGridworld\n",
    "from hr_coordination.mdp.overcooked_env import OvercookedEnv\n",
    "from hr_coordination.human.process_data import save_npz_file, get_trajs_from_data\n",
    "from hr_coordination.ppo.ppo import load_training_data, get_ppo_agent\n",
    "from hr_coordination.planning.planners import NO_COUNTERS_PARAMS, MediumLevelPlanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC_MODELS_TRAINING_BEST = {\n",
    "    \"simple\": (\"simple_training_nr2\", None), ##\n",
    "    \"unident_s\": (\"unident_s_training_nr2\", None),\n",
    "    \"random1\": (\"random1_training_nr1\", None),\n",
    "    \"random0\": (\"random0_training_nr3\", None),\n",
    "    \"random3\": (\"random3_training_nr0\", None)\n",
    "}\n",
    "\n",
    "BC_MODELS_TESTING_BEST = {\n",
    "    \"simple\": (\"simple_training_nr4\", None),\n",
    "    \"unident_s\": (\"unident_s_training_nr2\", \"acc\"),\n",
    "    \"random1\": (\"random1_training_nr3\", None),\n",
    "    \"random0\": (\"random0_training_nr3\", None),\n",
    "    \"random3\": (\"random3_training_nr0\", None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_params = DEFAULT_ENV_PARAMS.copy()\n",
    "env_params['ENV_HORIZON'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delivery_horizon(layout):\n",
    "    if layout == \"simple\" or layout == \"random1\":\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "def P_BC_evaluation_for_layout(layout):\n",
    "    env_params[\"FIXED_MDP\"] = layout\n",
    "    delivery_horizon = get_delivery_horizon(layout)\n",
    "    print(delivery_horizon)\n",
    "    \n",
    "    train_model_name, train_best_type = BC_MODELS_TRAINING_BEST[layout]\n",
    "    agent_bc_train, _, _ = get_bc_agent_from_saved(train_model_name, train_best_type)\n",
    "    agent_bc_train.stochastic = False\n",
    "    \n",
    "    agent_bc_train_embedded, _, _ = get_bc_agent_from_saved(train_model_name, train_best_type)\n",
    "    p_bc_train = EmbeddedPlanningAgent(agent_bc_train_embedded, agent_bc_train_embedded.mlp, delivery_horizon)\n",
    "    p_bc_train.env = OvercookedEnv.from_config(env_params)\n",
    "    p_bc_train.debug = True\n",
    "    \n",
    "    test_model_name, test_best_type = BC_MODELS_TESTING_BEST[layout]\n",
    "    agent_bc_test, _, _ = get_bc_agent_from_saved(test_model_name, test_best_type)\n",
    "    agent_bc_test.stochastic = False\n",
    "    \n",
    "    agent_bc_test_embedded, _, _ = get_bc_agent_from_saved(test_model_name, test_best_type)\n",
    "    p_bc_test = EmbeddedPlanningAgent(agent_bc_test_embedded, agent_bc_test_embedded.mlp, delivery_horizon)\n",
    "    p_bc_test.env = OvercookedEnv.from_config(env_params)\n",
    "    p_bc_test.debug = True\n",
    "    \n",
    "    # P_BC_test + BC_test\n",
    "    ave = AgentEvaluator.from_config(env_params)\n",
    "    ap_training = AgentPair(p_bc_test, agent_bc_train)\n",
    "    data = ave.evaluate_agent_pair(ap_training, num_games=1)\n",
    "    rew0 = data['ep_returns'][0]\n",
    "    \n",
    "    ave = AgentEvaluator.from_config(env_params)\n",
    "    ap_training = AgentPair(agent_bc_train, p_bc_test)\n",
    "    data = ave.evaluate_agent_pair(ap_training, num_games=1)\n",
    "    rew1 = data['ep_returns'][0]\n",
    "    print(\"P_BC_test + BC_test\", rew0, rew1)\n",
    "    \n",
    "    # P_BC_train + BC_test\n",
    "    ave = AgentEvaluator.from_config(env_params)\n",
    "    ap_testing = AgentPair(p_bc_train, agent_bc_test)\n",
    "    data = ave.evaluate_agent_pair(ap_testing, num_games=1)\n",
    "    rew0 = data['ep_returns'][0]\n",
    "    \n",
    "    ave = AgentEvaluator.from_config(env_params)\n",
    "    ap_testing = AgentPair(agent_bc_test, p_bc_train)\n",
    "    data = ave.evaluate_agent_pair(ap_testing, num_games=1)\n",
    "    rew1 = data['ep_returns'][0]\n",
    "    print(\"P_BC_train + BC_test\", rew0, rew1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layouts = ['simple', 'unident_s', 'random1']\n",
    "layouts = ['random1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "WARNING:tensorflow:From /Users/micah/miniconda3/envs/hrc/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/micah/Developer/Research/CHAI/hr_coordination/stable-baselines/stable_baselines/common/policies.py:436: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/micah/miniconda3/envs/hrc/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/micah/miniconda3/envs/hrc/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Loaded MediumLevelPlanner from data/planners/random1_am.pkl\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "Loaded MediumLevelPlanner from data/planners/random1_am.pkl\n",
      "{'pot-delivery': 5, 'dish-pot': 4}\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "Loaded MediumLevelPlanner from data/planners/random1_am.pkl\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MediumLevelPlanner from data/planners/random1_am.pkl\n",
      "{'pot-delivery': 5, 'dish-pot': 4}\n",
      "X X X P X \n",
      "X   ↑0  P \n",
      "D ↑1X   X \n",
      "O       X \n",
      "X O S X X \n",
      "Current orders: 20/20 are any's\n",
      "\n",
      "Found goal after: \t 971.9179887771606 seconds, \t 5763 state expanded (0.6559083810515357 frac. unique) \t ~5.929512640516966 expansions/s\n",
      "expected joint action ((-1, 0), (0, 0))\n",
      "Timestep: 1\n",
      "Joint action: ((-1, 0), (0, 0)) \t Reward: 0 + shape * 0 \n",
      "X X X P X \n",
      "X ←0    P \n",
      "D ↑1X   X \n",
      "O       X \n",
      "X O S X X \n",
      "Current orders: 20/20 are any's\n",
      "\n",
      "Found goal after: \t 766.1907057762146 seconds, \t 4521 state expanded (0.6509621765096217 frac. unique) \t ~5.900619735944007 expansions/s\n",
      "expected joint action ((1, 0), (0, -1))\n",
      "Timestep: 2\n",
      "Joint action: ((1, 0), (0, -1)) \t Reward: 0 + shape * 0 \n",
      "X X X P X \n",
      "X ↑1→0  P \n",
      "D   X   X \n",
      "O       X \n",
      "X O S X X \n",
      "Current orders: 20/20 are any's\n",
      "\n",
      "Found goal after: \t 573.1503758430481 seconds, \t 5553 state expanded (0.6509994597514857 frac. unique) \t ~9.688556850079843 expansions/s\n",
      "expected joint action ((1, 0), (0, 0))\n",
      "Timestep: 3\n",
      "Joint action: ((1, 0), (0, 0)) \t Reward: 0 + shape * 0 \n",
      "X X X P X \n",
      "X ↑1  →0P \n",
      "D   X   X \n",
      "O       X \n",
      "X O S X X \n",
      "Current orders: 20/20 are any's\n",
      "\n",
      "Found goal after: \t 485.086816072464 seconds, \t 5670 state expanded (0.6640211640211641 frac. unique) \t ~11.68862935898261 expansions/s\n",
      "expected joint action ((0, 1), (0, 0))\n",
      "Timestep: 4\n",
      "Joint action: ((0, 1), (0, 0)) \t Reward: 0 + shape * 0 \n",
      "X X X P X \n",
      "X ↑1    P \n",
      "D   X ↓0X \n",
      "O       X \n",
      "X O S X X \n",
      "Current orders: 20/20 are any's\n",
      "\n",
      "Found goal after: \t 729.8690528869629 seconds, \t 4521 state expanded (0.6509621765096217 frac. unique) \t ~6.194261809179874 expansions/s\n",
      "expected joint action ((0, 1), (0, 0))\n",
      "Timestep: 5\n",
      "Joint action: ((0, 1), (0, 0)) \t Reward: 0 + shape * 0 \n",
      "X X X P X \n",
      "X ↑1    P \n",
      "D   X   X \n",
      "O     ↓0X \n",
      "X O S X X \n",
      "Current orders: 20/20 are any's\n",
      "\n",
      "Found goal after: \t 589.1769242286682 seconds, \t 5429 state expanded (0.6456069257690182 frac. unique) \t ~9.214549614459994 expansions/s\n",
      "expected joint action ((-1, 0), (0, 1))\n",
      "Timestep: 6\n",
      "Joint action: ((-1, 0), (0, 1)) \t Reward: 0 + shape * 0 \n",
      "X X X P X \n",
      "X       P \n",
      "D ↓1X   X \n",
      "O   ←0  X \n",
      "X O S X X \n",
      "Current orders: 20/20 are any's\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-81a7a2c05904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayouts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mP_BC_evaluation_for_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-96e2bed37df7>\u001b[0m in \u001b[0;36mP_BC_evaluation_for_layout\u001b[0;34m(layout)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentEvaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0map_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_bc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_bc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_agent_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mrew0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ep_returns'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/agents/benchmarking.py\u001b[0m in \u001b[0;36mevaluate_agent_pair\u001b[0;34m(self, agent_pair, num_games, display)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_agent_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0magent_pair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# TODO: move this to PBT code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/mdp/overcooked_env.py\u001b[0m in \u001b[0;36mget_rollouts\u001b[0;34m(self, agent_pair, num_games, display, displayEnd, processed, final_state, agent_idx, reward_shaping)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0magent_pair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mtrajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtot_rews_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtot_rews_shaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayEnd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplayEnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/mdp/overcooked_env.py\u001b[0m in \u001b[0;36mrun_agents\u001b[0;34m(self, agent_pair, display, displayEnd, final_state, joint_actions, displayUntil)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0ms_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0ma_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_pair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;31m# Break if either agent is out of actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/agents/agent.py\u001b[0m in \u001b[0;36mjoint_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/agents/agent.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mml_s_a_plan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA_star_graph_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A* failed, taking random action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/planning/search.py\u001b[0m in \u001b[0;36mA_star_graph_search\u001b[0;34m(self, info)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackwards_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0msuccessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# if len(successors) == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/agents/agent.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mother_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mexpand_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_successor_states_fixed_other\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mother_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_agent_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mgoal_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mheuristic_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/planning/planners.py\u001b[0m in \u001b[0;36mget_successor_states_fixed_other\u001b[0;34m(self, start_state, other_agent, other_agent_idx)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mml_action\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mml_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# print(\"Expanding \", ml_action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0maction_plan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedded_low_level_action_plan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mml_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_agent_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/planning/planners.py\u001b[0m in \u001b[0;36mget_embedded_low_level_action_plan\u001b[0;34m(self, state, goal_pos_and_or, other_agent, other_agent_idx)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0msearch_problem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSearchTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheuristic_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         \u001b[0mstate_action_plan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA_star_graph_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m         \u001b[0maction_plan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_plan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstate_action_plan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0maction_plan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_plan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/planning/search.py\u001b[0m in \u001b[0;36mA_star_graph_search\u001b[0;34m(self, info)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackwards_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0msuccessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# if len(successors) == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/planning/planners.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0magent_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mother_agent_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m         \u001b[0mexpand_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedded_mdp_succ_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         \u001b[0mgoal_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_and_or\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgoal_pos_and_or\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0mheuristic_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_pos_and_or\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/planning/planners.py\u001b[0m in \u001b[0;36membedded_mdp_succ_fn\u001b[0;34m(self, state, other_agent)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0msuccessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_ACTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             \u001b[0msuccessor_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedded_mdp_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_agent_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0msuccessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccessor_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/planning/planners.py\u001b[0m in \u001b[0;36membedded_mdp_step\u001b[0;34m(self, state, action, other_agent_action, other_agent_index)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0mjoint_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_agent_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transition_states_and_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             \u001b[0msuccessor_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Env is deterministic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/mdp/overcooked_mdp.py\u001b[0m in \u001b[0;36mget_transition_states_and_probs\u001b[0;34m(self, state, joint_action)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;31m# Additional dense reward logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mpot_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pot_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mready_pots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpot_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tomato\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpot_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"onion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mcooking_pots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mready_pots\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpot_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tomato\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cooking\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpot_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"onion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cooking\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Research/CHAI/hr_coordination/hr_coordination/mdp/overcooked_mdp.py\u001b[0m in \u001b[0;36mget_pot_states\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_items\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                     \u001b[0mpots_states_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msoup_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'one_onion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpot_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                 \u001b[0;32melif\u001b[0m \u001b[0mnum_items\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m                     \u001b[0mpots_states_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msoup_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'two_onion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpot_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mnum_items\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for layout in layouts:\n",
    "    P_BC_evaluation_for_layout(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P_BC with horizon 100\n",
    "\n",
    "simple\n",
    "P_BC_test + P_BC_test: 40, 40 -> 160\n",
    "P_BC_train + BC_test: 40, 40 -> 160\n",
    "\n",
    "unident_s\n",
    "P_BC_test + P_BC_test: 80, 60 -> 280\n",
    "P_BC_train + BC_test: 60, 40 -> 200\n",
    "\n",
    "random1 (two delivery horizon)\n",
    "P_BC_test + P_BC_test: \n",
    "P_BC_train + BC_test: \n",
    "\n",
    "-----\n",
    "\n",
    "180, 180\n",
    "P_BC_train + BC_test: 180, 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
